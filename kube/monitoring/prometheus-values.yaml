# kube-prometheus-stack Helm values for OMS monitoring
#
# Install:
#   helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
#   helm repo update
#   helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
#     -n monitoring --create-namespace -f kube/monitoring/prometheus-values.yaml

prometheus:
  prometheusSpec:
    retention: 15d
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: do-block-storage
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 20Gi
    # Discover ServiceMonitors across all namespaces
    serviceMonitorSelectorNilUsesHelmValues: false
    serviceMonitorNamespaceSelector: {}
    ruleSelectorNilUsesHelmValues: false
    ruleNamespaceSelector: {}
    podMonitorSelectorNilUsesHelmValues: false
    podMonitorNamespaceSelector: {}
    resources:
      requests:
        cpu: 200m
        memory: 512Mi
      limits:
        cpu: "1000m"
        memory: 2Gi

grafana:
  enabled: true
  adminPassword: "" # Set via --set grafana.adminPassword=<password> during install
  ingress:
    enabled: true
    ingressClassName: nginx
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-issuer
    hosts:
      - grafana.ordermysaddle.com
    tls:
      - secretName: grafana-tls
        hosts:
          - grafana.ordermysaddle.com
  persistence:
    enabled: true
    storageClassName: do-block-storage
    size: 5Gi
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: default
          orgId: 1
          folder: ""
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/default
  dashboards:
    default:
      # Node Exporter Full dashboard
      node-exporter:
        gnetId: 1860
        revision: 33
        datasource: Prometheus
      # Kubernetes cluster monitoring
      kubernetes-cluster:
        gnetId: 6417
        revision: 1
        datasource: Prometheus
      # NGINX Ingress Controller
      nginx-ingress:
        gnetId: 9614
        revision: 1
        datasource: Prometheus
  sidecar:
    dashboards:
      enabled: true
      searchNamespace: ALL

alertmanager:
  alertmanagerSpec:
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 200m
        memory: 256Mi
  config:
    global:
      resolve_timeout: 5m
    route:
      group_by: ["alertname", "namespace", "service"]
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 4h
      receiver: slack-notifications
      routes:
        - match:
            severity: critical
          receiver: slack-critical
          repeat_interval: 1h
    receivers:
      - name: "null"
      - name: slack-notifications
        slack_configs:
          - api_url: "" # Set via --set alertmanager.config.receivers[1].slack_configs[0].api_url=<webhook>
            channel: "#oms-alerts"
            send_resolved: true
            title: '{{ "{{ .Status | toUpper }}{{ if eq .Status \"firing\" }} ({{ .Alerts.Firing | len }}){{ end }}" }}'
            text: >-
              {{ "{{ range .Alerts }}" }}
              *Alert:* {{ "{{ .Annotations.summary }}" }}
              *Description:* {{ "{{ .Annotations.description }}" }}
              *Severity:* {{ "{{ .Labels.severity }}" }}
              *Namespace:* {{ "{{ .Labels.namespace }}" }}
              {{ "{{ end }}" }}
      - name: slack-critical
        slack_configs:
          - api_url: "" # Set via --set
            channel: "#oms-alerts-critical"
            send_resolved: true
            title: 'CRITICAL: {{ "{{ .Status | toUpper }}{{ if eq .Status \"firing\" }} ({{ .Alerts.Firing | len }}){{ end }}" }}'
            text: >-
              {{ "{{ range .Alerts }}" }}
              *Alert:* {{ "{{ .Annotations.summary }}" }}
              *Description:* {{ "{{ .Annotations.description }}" }}
              {{ "{{ end }}" }}

# Reduce resource usage for non-critical components
kubeStateMetrics:
  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      cpu: 200m
      memory: 256Mi

nodeExporter:
  resources:
    requests:
      cpu: 50m
      memory: 32Mi
    limits:
      cpu: 200m
      memory: 128Mi

prometheusOperator:
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 512Mi
